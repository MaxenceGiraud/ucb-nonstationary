The work discussed in this paper is focused on Non-Stationary bandit, as of now we have only seen Stationary bandits. In this problem the learner must decide which arm to play with the same reward system as in the classic Multi-armed bandit problem but facing the possibility of a changing environment.\\

This problem can be expressed in the following way. The rewards $X_s(i)$ of an arm $i$ are modeled by an independent sequence of random variables from a distribution that may change across time.\\

Garivier et al. focused their work on abruptly changing environment, meaning that the distributions of rewards remain constant during periods and change at unknown time instants called \textbf{breakpoints}. Other non Stationary bandits problems exist where the distributions of the rewards are changed continuously but are not the main topic of this paper. They nevertheless still try to see how the different policies perform in both types of non stationary environments.\\

It is straighforward that standard policies such as UCB reviewed in the last subsection are not appropriate for this kind of environment, this is why the authors compare a new algorithm of their own Sliding Window UCB with an existing one Discounted UCB. Those two policies are going to be the main focus of this report.